\documentclass{article}

\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fullpage}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Diag}{Diag}

\begin{document}

Each section is a new lecture.

\section{Applications}
\begin{itemize}
\item slides on applications of machine learning.
\item calculus/programming quiz.
\end{itemize}

\section{Training data in supervised learning}

Training data: $D= \{(x_1,y_1), \dots, (x_n,y_n)\}$, given/known.

Inputs $x_i\in\mathcal X$, outputs
$y_i\in\mathcal y$.

For each example, what is $\mathcal X,\mathcal Y$? 
\begin{itemize}
\item Image classification
\item Image segmntation
\item Translation
\item Spam filtering
\end{itemize}

Usually $\mathcal X = \mathbb R^p$ (e.g. word counts in emails).

Notation. 
\begin{itemize}
\item Inputs $x\in\mathbb R^p$.
\item $p$ = dimension of each input.
\item $n$ = number of training examples.
\item Outputs $y\in\mathbb R$ for regression, $y\in\{0,1\}$ for binary
  classification.
\item $f:\mathcal X\rightarrow \mathcal Y$ is the prediction function
  we want to learn.
\item Regression function $f:\mathbb R^p\rightarrow\mathbb R$
\item Binary classification function $f:\mathbb R^p\rightarrow \{0,1\}$.
\end{itemize}

Geometric interpretation of regression, height son/father (linear
pattern), species versus temperature (non-linear pattern).

Draw residual $r_i$ on graphs as vertical line segments.

\paragraph{Test data.} $D'= \{ (x'_1,y'_1), \dots, (x'_m,y'_m)\}$, unknown.
Goal is generalization to new/test data: algo $\textsc{Learn}(D) = f$,
with $f(x'_i)\approx y'_i$ for all test data, i.e. minimize
\begin{equation}
  \text{Err}_{D'}(f) = \sum_{(x',y')\in D'} \ell[f(x'), y']
\end{equation}
where $\ell$ is a loss function:
\begin{itemize}
\item In binary classification we typically use the
  mis-classification-rate/0-1-loss $\ell[f(x),y]=I[c_f(x)!=y]$, where
  the binary classifier $c_f(x)=0$ if $f(x)<0.5$ and 1 otherwise.
\item In regression we use the squared error $\ell[f(x),y]=[f(x)-y]^2$.
\end{itemize}
Since $D'$ is unknown, we need to assume that $D,D'\sim \mathcal P$.

\paragraph{Nearest neighbors algorithm.} What is near?
$\textsc{Dist}:\mathcal X\times \mathcal X \rightarrow \mathbb R_+$ is
a non-negative distance function between two data points.

e.g. for $\mathcal X=\mathbb R^p$ we use
\begin{itemize}
\item L1 distance $\textsc{Dist}(x,x') = ||x-x'||_1 = \sum_{j=1}^p |x_j-x_j'|$
\item L2 distance $\textsc{Dist}(x,x') = ||x-x'||_2 = \sqrt{
\sum_{j=1}^p (x_j-x_j')^2}$
\end{itemize}
Draw geometric interpretation of L1 and L2 distances.

Compute distances $d_1,\dots,d_n\in\mathbb R_+$ where 
$d_i=\textsc{Dist}(x_i,x)$.

Example graph, draw horizontal line segments $d_i$ between $x,x_i$.

Define neighbors function $N_{D,K}(x') = \{t_1, t_2, \dots, t_K\}$, where\\
$t_1,\dots,t_n\in\{1,\dots,n\}$ are indices such that\\
distances $d_{t_1}\leq \cdots \leq d_{t_n}$ are sorted ascending.

Predict the mean output value of the $K$ nearest
neighbors, $$f_{D,K}(x') = \frac 1 K \sum_{i\in N_{D,K}(x')} y_i$$

\begin{itemize}
\item For regression this is the mean.
\item For binary classification the mean is intepreted as a
  probability, predict 1 if greater than 0.5, and predict 0 otherwise.
\end{itemize}

\section{Nearest neighbors model selection and code}
\begin{itemize}
\item Review of supervised learning.
\item Definition of nearest neighbors prediction function $f_{D,K}(x)$.  
\item How to choose $K$? Discussion.
\item Formal definition of total error.
\item Visualizations. 1d regression and binary classification.
\item Draw train/validation matrices. Validation error.
\item Pseudocode for computing $f_{D,K}(x)$.
\end{itemize}

\begin{equation}
  \mathcal L_{D,D'}(k) = \sum_{(x',y')\in D'} \ell[f_{D,k}(x'), y']
\end{equation}

\begin{algorithmic}[1]
  \State Function $\textsc{PredKNearestNeighbors}$
  \State Inputs: train inputs/features $x_1,\dots,x_n$, outputs/labels $y_1,\dots,y_n$,\\ \ \ \ test input/feature $x'$, max number of neighbors $K_{\text{max}}$:
  \For{$i=1$ to $n$}:
  \State $d_i\gets \textsc{Distance}(x', x_i)$
  \EndFor
  \State $t_1,\dots,t_n\gets \textsc{SortedIndices}(d_1,\dots,d_n)$
  \State $\text{totalY}\gets 0.0$
  \For{$k=1$ to $K_{\text{max}}$}:
  \State $i\gets t_k$
  \State $\text{totalY} \texttt{ += } y_i$
  \EndFor
  \State Output: prediction $\text{totalY}/K_{\text{max}}$
\end{algorithmic}
Total complexity: $O(np + n\log n)$ where the Distance sub-routine is $O(p)$.

\section{Cross-validation}

Interactive data viz.

Remember we assume that training data and test data are randomly drawn
from a common probability distribution $\mathcal P$. To simulate that
process, we randomly assign a fold ID for each observation
$z_1,\dots,z_n\in\{1,\dots, S_\text{max}\}$. We then define
$\text{NumFolds}$ train/validation splits
\begin{equation}
  D_{z=s} = \{(x_i,y_i)|z_i = s\} \text{ --- validation set $s$}
  D_{z\neq s} = \{(x_i,y_i)|z_i \neq s\} \text{ --- train set $s$}
\end{equation}
Draw train/validation/test matrices for $s=1$, $s=2$, etc.

Cross-validation data visualizations.

Mean validation error over all train/validation splits (folds):
\begin{equation}
  \text{MeanVerr}(k) = \mathcal L_{D_{z\neq s},D_{z=s}}(k)
\end{equation}

Overall we choose the parameter $\hat k = \argmin_k \text{MeanVerr}(k)$.

And the learning algorithm is $\textsc{LearnKNN}(D) = f_{D, \hat k}$.

\subsection{Pseudocode}

We assume there are sub-routines
\begin{itemize}
\item
  $\textsc{Pred1toKmaxNN}(D, x', K_{\text{max}}) = [ f_{D, 1}(x')
  ... f_{D, K_{\text{max}}}(x') ]$ -- all predictions from $k=1$ to
  $K_{\text{max}}$ neighbors for a single test point $x'$ based on the training
  data set $D$.
\item $\textsc{PredError}(D, D', K_{\text{max}})$ trains on $D$ and
  computes $\mathcal L_{D,D'}(k)$, the validation error wrt $D'$ for
  $k=1$ to $K_{\text{max}}$ neighbors.
\end{itemize}

Then the pseudo code for the learning algo is:

\begin{algorithmic}[1]
  \State Function $\textsc{LearnKNN}$
  \State Inputs: train data $D$ ($n$ observations in $p$ dimensions), 
  number of folds $S_{\text{max}}\in\{1,\dots, n\}$, 
  max number of neighbors $K_{\text{max}}\in\{1, \dots, n\}$.
  \State Randomly assign fold IDs $z\in\{1,\dots,S_{\text{max}}\}^n$
  \For{each fold $s=1$ to $S_{\text{max}}$}:
  \State $E_s\gets \textsc{PredError}(D_{z\neq s}, D_{z=s}, K_{\text{max}})\in\mathbb R^{K_{\text{max}}}$
  \EndFor
  Let
  $E\gets[E_1 ... E_{S_{\text{max}}}]\in\mathbb R^{K_{\text{max}}\times
    S_{\text{max}}}$ be the matrix of error/loss values for all
  folds/columns and neighbors/rows.
  \State $\text{MeanVerr}\gets \textsc{colMeans}(E)\in\mathbb R^{K_{\text{max}}}$
  \State The optimal number of neighbors is $\hat k = \argmin_k \text{MeanVerr}_k$
  \State return $f_{D,\hat k}$
\end{algorithmic}

\section{R package development}

Choose groups.

\section{Coding nearest neighbors prediction}

For one test point, demo code in C++.

\section{Linear regression / gradient descent}

Want to minimize squared error on test data, but we don't have them,
so instead we do
\begin{equation}
  \min_f \sum_{i=1}^n [f(x_i)-y_i]^2
\end{equation}
We parameterize a linear function using a weight vector $w\in\mathbb R^p$,
$f_w(x_i)=w^T x_i$. 

The input feature matrix is 
\begin{equation}
  X = \left[\begin{array}{c}
              x_1^T \\
              \vdots \\
              x_n^T
\end{array}\right]\in\mathbb R^{n\times p}
\end{equation}
So for a partcular weight vector $w$ the prediction vector is
\begin{equation}
  Xw = \left[\begin{array}{c}
              x_1^T w \\
              \vdots \\
              x_n^T w
\end{array}\right]\in\mathbb R^{n}
\end{equation}
So the least squares regression problem is
\begin{equation}
  \min_{w\in\mathbb R^p} || Xw - y||_2^2
\end{equation}
Notation about matrix/vector dimensions.

What is a norm $||\cdot||$ = size of a vector.

Squared L2 norm $||v||_2^2 = v^T v$.

L1 norm $||v||_1 = \sum_{j=1}^p |v_j|$.

Univariate linear regression: $p=2$, $x_i=[1\ x_{i,2}]$.
\begin{equation}
  X = \left[\begin{array}{cc}
              1 & x_{1,2} \\
              \vdots & \vdots \\
              1 & x_{n,2}
\end{array}\right]\in\mathbb R^{n\times 2}
\end{equation}
and
$f(x_i) = w^T x_i = \underbrace{w_1}_{\text{intercept}} +
\underbrace{w_2}_{\text{slope}} x_{i,2}$. Draw figure on board then
show data viz.

Gradient is defined for any $w$ as the direction of steepest ascent:
\begin{equation}
  \nabla \mathcal L(w) = \left[\begin{array}{c}
              \frac{d}{dw_1} \mathcal L(w)\\
              \vdots \\
              \frac{d}{dw_p} \mathcal L(w)\\
\end{array}\right]\in\mathbb R^{p}
\end{equation}
For $p=1$ we have 
\begin{equation}
  \nabla \mathcal L(w) = \mathcal L'(w) = \frac{d}{dw}\mathcal L(w) = 
  \sum_{i=1}^n 2x_i(w x_i - y_i).
\end{equation}
In general we have
\begin{equation}
  \nabla \mathcal L(w) = 
  \nabla_w ||Xw - y||_2^2 = 
  2X^T(Xw-y).
\end{equation}
Algorithm: $w^{(0)}=0$ be the initial guess.
Then for any stepsize $\alpha>0$ define update rules for any iteration $t>0$:
\begin{equation}
  w^{(t)} = w^{(t-1)} - \alpha \nabla \mathcal L(w^{(t-1)}).
\end{equation}
Example: gradient descent on $\mathcal L(w)=(x+5)^2$. Start at the
origin. Compute gradient, first step.

\section{Probabilistic interpretation of linear regression}

Definitions of argmin, min.
\begin{equation*}
  \argmin_{w\in\mathbb R^p}\mathcal L(w) = \{w^*\in\mathbb R^p|\mathcal L(w^*)\leq L(w_0) \forall w_0\in\mathbb R^p\}.
\end{equation*}
Draw quadratic function $x^2$ and $x^2+1$ which have the same argmin
but different min.

Definition of argmax: draw upside down quadratic function $-x^2$ to
show relationship between argmin and argmax.

Why do we need to do this derivation? 
\begin{enumerate}
\item For binary classification can't minimize zero-one loss via gradient
descent.
\item Using $w^T x_i\in\mathbb R$ directly for binary classification
  does not make sense.
\end{enumerate}

Begin by re-deriving the least squares problem, assuming labels are
normally distributed.

Let $y_i\sim N(w^T x_i, s^2)$, draw probability density function.
\begin{equation*}
  \Pr(y_i, w^T x_i, s^2) = (2\pi s^2)^{-1/2} \exp\left[
\frac{-1}{2s^2}(y_i-w^Tx_i)^2
\right]
\end{equation*}
Let the likelihood of $w$ be
\begin{equation*}
  \Lik(w) = \prod_{i=1}^n \Pr(y_i, w^T x_i, s^2)
\end{equation*}
Then the log-likelihood is
\begin{equation*}
  \log\Lik(w) = \sum_{i=1}^n \log\Pr(y_i, w^T x_i, s^2)
\end{equation*}
Then we have 
\begin{eqnarray*}
  \argmax_w \log\Lik(w) 
&=& \argmax_w \sum_{i=1}^n \underbrace{
\frac{-1}{2}\log(2\pi s^2)
}_{\text{constant wrt $w$}} -
\frac{1}{2s^2}(y_i-w^Tx_i)^2\\
&=& \argmax_w -\sum_{i=1}^n
\underbrace{
\frac{1}{2s^2}
}_{\text{cst}}(y_i-w^Tx_i)^2\\
&=& \argmin_w \sum_{i=1}^n
(y_i-w^Tx_i)^2\\
\end{eqnarray*}

For binary labels assume Bernoulli distribution. Exercise: Derive
logistic loss, assuming $\tilde y_i\in\{-1,1\}$.

\section{Logistic regression}

Assume $y_i\sim \text{Bernoulli}(p_i)$ with
$p_i = \sigma(w^T x_i)\in(0,1)$ and
\begin{equation*}
  \sigma(z) = \frac{1}{1+e^{-z}}.
\end{equation*}
The probability mass function is
\begin{equation*}
  \Pr(y_i, p_i) =
  \begin{cases}
    p_i & \text{ if } y_i=1\\
    1-p_i & \text{ if } y_i=0.
  \end{cases}
\end{equation*}
The log-likelihood is then
\begin{eqnarray*}
  \log\Lik(w) &=& \sum_{i=1}^n\log \Pr(y_i, \sigma(w^T x_i))\\
&=& \sum_{i=1}^n \log[\sigma(w^T x_i)] I(y_i=1) + \log[1-\sigma(w^T x_i)] I(y_i=0)\\
&=& \sum_{i=1}^n \log[\frac{1}{1+\exp(-w^T x_i)}] I(y_i=1) + 
\log[\underbrace{1-\frac{1}{1+\exp(-w^T x_i)}}_{
\frac{\exp(-w^T x_i)}{1+\exp(-w^T x_i)}
}] I(y_i=0)\\
&=& \sum_{i=1}^n \log[\frac{1}{1+\exp(-w^T x_i)}] I(\tilde y_i=1) + 
\log[\frac{1}{1+\exp(w^T x_i)}] I(\tilde y_i=-1)\\
&=& \sum_{i=1}^n \log[\frac{1}{1+\exp(-\tilde y_i w^T x_i)}]\\
&=& \sum_{i=1}^n -\log[1+\exp(-\tilde y_i w^T x_i)]
\end{eqnarray*}
where the alternate labels are 
\begin{equation*}
  \tilde y_i =
  \begin{cases}
    -1 & \text{ if } y_i=1\\
    1 & \text{ if } y_i=0.
  \end{cases}
\end{equation*}
The logistic loss function that we want to minimize is thus
\begin{equation*}
  \mathcal L(w) = -\log\Lik(w) = \sum_{i=1}^n 
\underbrace{\log[1+\exp(-\tilde y_i w^T x_i)]}_{
\ell[w^T x_i, \tilde y_i]
}.
\end{equation*}
The gradient is
\begin{equation*}
\nabla_w \ell[w^T x_i, \tilde y_i] = \frac{
  -\tilde y_i x_i\exp(-\tilde y_i w^T x_i)
}{
  1+\exp(-\tilde y_i w^T x_i)
}=\frac{
  -\tilde y_i x_i
}{
  1+\exp(\tilde y_i w^T x_i)
}
\end{equation*}
Draw the logistic loss for $y_i=1$ as a function of $w^T x_i$. 
\begin{eqnarray*}
\ell[w^T x_i, \tilde y_i] &=& \log[1+\exp(- w^T x_i)] \\
\nabla_{w^T x_i} \ell[w^T x_i, \tilde y_i] &=& \frac{
  - 1
}{
  1+\exp( w^T x_i)
}
\end{eqnarray*}
\begin{itemize}
\item As $w^T x_i\rightarrow \infty$ we have
  $\exp(-w^T x_i)\rightarrow 0$ and
  $\log [1+\exp(- w^T x_i)]\rightarrow 0$.
\item As $w^T x_i\rightarrow -\infty$ we have
  $\exp(-w^T x_i)\rightarrow \infty$,
  $\ell[w^T x_i, \tilde y_i] = \log [1+\exp(- w^T x_i)]\rightarrow \infty$,
  $\exp(w^T x_i)\rightarrow 0$, and $\nabla_{w^T x_i} \ell[w^T x_i, \tilde y_i] \rightarrow -1$.
\end{itemize}


The gradient $\nabla\mathcal L:\mathbb R^p\rightarrow\mathbb R^p$ is
\begin{equation*}
\nabla \mathcal L(w) = \sum_{i=1}^n \nabla_w \ell[w^T x_i, \tilde y_i] = 
\sum_{i=1}^n \frac{
  -\tilde y_i x_i
}{
  1+\exp(\tilde y_i w^T x_i)
} = -X^T \tilde Y S(\tilde Y X w),
\end{equation*}
where $\tilde Y=\Diag(\tilde y)$ and $S:\mathbb R^n\rightarrow R^n$ is the componentwise application
of the logistic link function,
$S(v) = \left[
    \begin{array}{ccc}
      \sigma(v_1) & \cdots & \sigma(v_n)
    \end{array}
\right]^T$.

Discuss gradient descent algorithm, same as least squares regression.

Discuss learning algorithm which chooses the number of steps (early
stopping) that minimizes the mean validation loss from CV.

\section{L2 regularization}

TODO

\end{document}
