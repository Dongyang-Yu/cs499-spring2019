Computer Science 499-007 (9235) at Northern Arizona University, Spring 2019

Optimization Algorithms for Machine Learning

TuTh 9:35-10:50

## Schedule

Jan 14 to May 10.

```r

all.day.vec <- seq(as.Date("2019-01-14"), as.Date("2019-05-10"), by = "day")
class.day.vec <- all.day.vec[strftime(all.day.vec, "%a") %in% c("Tue", "Thu")]
lecture.vec <- paste(
  "* Lecture", seq_along(class.day.vec), strftime(class.day.vec, "%a %b %d"))
cat(lecture.vec, sep="\n")

```

Classification data sets
* ElemStatLearn::spam 2-class [4601, 57] output is last column (spam).
* ElemStatLearn::SAheart 2-class [462, 9] output is last column (chd).
* ElemStatLearn::zip.train: 10-class [7291, 256] output is first column.
* [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist):
  10-class [60000 x 784] Read into R via https://gist.github.com/brendano/39760
  
Regression data sets
* ElemStatLearn::prostate [97 x 8] output is lpsa column, ignore train column.
* ElemStatLearn::ozone [111 x 3] output is first column (ozone).
* nycflights13::flights, output is arr_delay.

```r
data(zip.train, package="ElemStatLearn")
library(data.table)
zip.wide <- data.table(zip.train)
setnames(zip.wide, c(
  "digit", paste(rep(1:16, 16), rep(1:16, each=16))))
zip.wide[, example.i := 1:.N]
zip.tall <- melt(zip.wide, id.vars=c("example.i", "digit"))
zip.tall[, row := as.integer(sub(".* ", "", variable))]
zip.tall[, col := as.integer(sub(" .*", "", variable))]
zip.some <- zip.tall[example.i %in% c(1:10)]

library(ggplot2)
ggplot()+
  facet_wrap("example.i")+
  theme_bw()+
  theme(panel.margin=grid::unit(0, "lines"))+
  geom_tile(aes(
    col, -row, fill=value),
    data=zip.some)+
  scale_fill_gradient(low="white", high="black")
```

Coding projects:
* lasso solver
* SVM solver
* k-means / GMM / hierarchical clustering

Topics:
* Support vector machines
* Neural networks
* Decision tree models
* Nearest neighbors
* K-means
* Gaussian mixture models
* Hierarchical clustering 
* Hidden Markov models
* Optimal segmentation / dynamic programming
   
Murphy Chapter 1
- Logistic regression
- Overfitting / model selection

Murphy Chapter 16
* 16.5 Feedforward neural networks

LP/QP solver examples
* Murphy 7.4 Robust linear regression
* Lasso
* SVM

Murphy Chapter 7 
* 7.5 Ridge regression.

Murphy Chapter 8
* 8.3.2 Steepest descent
* 8.3.3 Newton's method
* 8.3.5 Quasi-newton methods
* 8.3.6 L2 regularization
* 8.3.7 Multi-class logistic regression


Murphy Chapter 13 
* Linear models
* Best subset selection 
* Lasso
* Coordinate descent
* LARS
* Proximal and gradient projection methods

Applications
* Music transcription
* Translation
* Spam filtering
* Medical diagnosis
* Optical/intelligent character/word recognition
* Image classification
* Image segmentation
* Genomic segmentation
 
* Lecture 1 Tue Jan 15: Applications of machine learning, syllabus, placement test
* Lecture 2 Thu Jan 17: Nearest neighbors, cross-validation
* Lecture 3 Tue Jan 22: Linear regression, gradient descent
* Lecture 4 Thu Jan 24: Ridge regression
* Lecture 5 Tue Jan 29: Writing an R package
* Lecture 6 Thu Jan 31: C/C++ code in R packages via .C
* Lecture 7 Tue Feb 05: Multi-class, L2-regularized, logistic regression
* Lecture 8 Thu Feb 07: stochastic gradient
* Lecture 9 Tue Feb 12: Neural networks
* Lecture 10 Thu Feb 14: backpropagation algorithm.

Coding project 1. R package for smooth optimization, linear/logistic
regression, compare L2 regularization with early stopping.

* Lecture 11 Tue Feb 19: Best subset selection, L1-regularization
* Lecture 12 Thu Feb 21: proximal gradient
* Lecture 13 Tue Feb 26: coordinate descent
* Lecture 14 Thu Feb 28: LARS algorithm.
* Lecture 15 Tue Mar 05: SVM Primal 
* Lecture 16 Thu Mar 07: SVM Dual 
* Lecture 17 Tue Mar 12: kernel trick
* Lecture 18 Thu Mar 14: Sequential Minimal Optimization

Coding project 2. R package for non-smooth optimization, Lasso and
SVM.

MIDTERM EXAM on smooth optimization algorithms for supervised learning

* Lecture 19 Tue Mar 19: Hierarchical clustering.
* Lecture 20 Thu Mar 21: K-means clustering.
* Lecture 21 Tue Mar 26: Gaussian mixture models.
* Lecture 22 Thu Mar 28: Other mixture models.
* Lecture 23 Tue Apr 02: Hidden Markov Models.
* Lecture 24 Thu Apr 04: Forward-backward algorithm. 
* Lecture 25 Tue Apr 09: Viterbi, Baum-Welch.
 * Lecture 26 Thu Apr 11: Optimal segmentation, dynamic programming.

Decision trees, CART,Random forests?

Coding project 3. R package for unsupervised learning

* Lecture 27 Tue Apr 16: paper review?
* Lecture 28 Thu Apr 18: 
* Lecture 29 Tue Apr 23: 
* Lecture 30 Thu Apr 25: 
* Lecture 31 Tue Apr 30: 
* Lecture 32 Thu May 02:
* Lecture 33 Tue May 07:
* Lecture 34 Thu May 09:

Coding project 4. R package for algo from ML literature.
- 

FINAL EXAM on non-smooth, discrete, unsupervised algorithms

## Books

* https://www.cs.ubc.ca/~murphyk/MLbook/
* NAU library https://arizona-nau -primo.hosted.exlibrisgroup.com/primo-explore/fulldisplay?docid=01NAU_ALMA51166833980003842&context=L&vid=01NAU&lang=en_US&search_scope=Everything&adaptor=Local%20Search%20Engine&tab=default_tab&query=any,contains,murphy%20machine%20learning&sortby=rank&mode=Basic
* e-book http://eds.a.ebscohost.com/ehost/ebookviewer/ebook?sid=f4a68ba6-099a-4015-9d63-a342771786f3%40sdc-v-sessmgr02&vid=0&format=EB
* http://numerical.recipes/
* https://web.stanford.edu/~hastie/ElemStatLearn/
