\documentclass{article}

\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{setspace}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Diag}{Diag}

\begin{document}

\section{2019-01-15-applications}
Let $z_1, \dots, z_n\in\mathbb Z_+$ be a set of $n$ non-negative integers,
i.e. $\mathbb Z_+=\{0, 1, 2, \dots\}$. Define for any positive $\mu>0$ the function
$$ f(\mu) = \sum_{i=1}^n \mu - z_i\log \mu, $$
where $\sum_{i=1}^n$ denotes the sum of $n$ terms, and $\log$ is the
natural logarithm ($\log=\ln$).

\begin{enumerate}
\item Derive an expression in terms of $\mu,n,z_i$ for the first derivative
  $f'(\mu)$. 

\vskip 1in
\item Derive an expression in terms of $n,z_i$ for the value of $\mu$
  which is a critical point of $f$. Hint: set $f'(\mu)=0$ and solve
  for $\mu$.

\vskip 1in
\item If $n=3$ and $z_1=1, z_2=0, z_3=2$, then compute the value of
  the critical point $\mu=\underline{\hspace{1cm}}$ and the critical
  function value $f(\mu)=\underline{\hspace{1cm}}$. Fill in the blanks
  here but show your work below.

\vskip 1in
\item Is the critical point a minimum ($f''(\mu)>0$), maximum
  ($f''(\mu)<0$), or inflection point ($f''(\mu)=0$)?

\vskip 1in
\item Write a function in the C programming language that computes the
  critical point $\mu$. The function should have two input arguments:
  the number $n$ of data \texttt{int n} and a pointer \texttt{int* z}
  to the data $z_i$. The function should output the critical point
  $\mu$ as a \texttt{double}.
\end{enumerate}

\newpage
\section{2019-01-17-nearest-neighbors}
Here are $n=5$ data with $p=2$ input dimensions. Each row is a person
for which we have measured the height (first column of $X$, in
centimeters), weight (second column in pounds). The output $y$ that we
want to predict is diabetes diagnosis status (1=diabetes, 0=not).

\begin{equation*}
  X = \left[\begin{array}{cc}
              170 & 140 \\
              200 & 160 \\
              180 & 200 \\
              140 & 150 \\
              150 & 130
\end{array}\right],\ 
  y = \left[\begin{array}{c}
              0 \\
              0 \\
              1 \\
              1 \\
              1
\end{array}\right],\ 
  d = \left[\begin{array}{c}
              \underline{\hspace{1cm}}\\
              \underline{\hspace{1cm}}\\
              \underline{\hspace{1cm}}\\
              \underline{\hspace{1cm}}\\
              \underline{\hspace{1cm}}
\end{array}\right]
\end{equation*}

The goal is to compute the class predicted by the $K=3$ nearest
neighbor model, for a new/test person with features $x$=[height = 160 cm,
weight = 130 pounds].

Assume that we use the Manhattan/L1 distance metric, 
\begin{equation*}
  d(x,x') = \sum_{j=1}^p |x_j-x_j'|,
\end{equation*}
i.e. the total distance between $x,x'$ is the sum of distances on each
of the two component dimensions (height and weight).

\begin{enumerate}
\item Fill in the blanks in the vector of distances $d$ above (each
  row should be the Manhattan/L1 distance between the new/test person
  and the training data).
\item Which are the three nearest neighbors? (write a star* for each
  of the three nearest neighbors in the blank next to the
  corresponding distances/rows)
\item What is the overall predicted class? (0 or 1)
\end{enumerate}


\newpage
\section{2019-01-22-pseudocode}
Below I have written pseudo-code for a version of the $k$-nearest
neighbors algorithm. Fill in the blank on line 12 so that the
algorithm computes predictions $\hat y_k$ for all
$k\in\{1,\dots, K_{\text{max}}\}$.

\begin{algorithmic}[1]
  \State Function $\textsc{Pred1toKmaxNearestNeighbors}$
  \State Inputs: train inputs/features $x_1,\dots,x_n$, outputs/labels $y_1,\dots,y_n$,\\ \ \ \ test input/feature $x'$, max number of neighbors $K_{\text{max}}$:
  \For{$i=1$ to $n$}:
  \State $d_i\gets \textsc{Distance}(x', x_i)$
  \EndFor
  \State $t_1,\dots,t_n\gets \textsc{SortedIndices}(d_1,\dots,d_n)$
  \State $\text{totalY}\gets 0.0$
  \For{$k=1$ to $K_{\text{max}}$}:
  \State $i\gets t_k$
  \State $\text{totalY} \texttt{ += } y_i$
  \State $\hat y_k\gets \underline{\hspace{1in}}$
  \EndFor
  \State Output: predictions $\hat y_1, \dots, \hat y_{K_\text{max}}$.
\end{algorithmic}

\newpage
\section{2019-01-24-cross-validation}
The image below represents a training data set with $n=70$
observations, one for each individual image of a digit. In order to
perform cross-validation, fold ID numbers $\in\{1,2,3\}$ have been
assigned to all observations/images in the corresponding row/letter.

%  Assume you
% are given the corresponding matrix of pixel intensities,
% $X\in\mathbb R^{70\times 256}$ (each row is an observation/image, each
% column is the intensity of one pixel -- larger values are whiter and
% smaller values are darker). Further assume you are given the
% corresponding outputs/labels $y\in\{0,1,\dots,9\}^n$.

\newcommand{\myskip}{\vskip 0.8cm}
\parbox{0.1\textwidth}{
A, fold=1
\myskip
B, fold=2
\myskip
C, fold=2
\myskip
D, fold=1
\myskip
E, fold=3
\myskip
F, fold=3
\myskip
G, fold=3
}
\parbox{0.9\textwidth}{
  \includegraphics[width=0.9\textwidth]{mnist-digits}
}

\begin{enumerate}
\vskip 1cm
\item For fold/split 1 which observations/letters are used for the
  training set?  \underline{\hspace{1.5in}} Which observations/letters
  are used for validation set? \underline{\hspace{1.5in}}

\vskip 1cm
\item For fold/split 2. Training set = \underline{\hspace{1.5in}},
  Validation set = \underline{\hspace{1.5in}}.

\vskip 1cm
\item For fold/split 3. Training set = \underline{\hspace{1.5in}},
  Validation set = \underline{\hspace{1.5in}}.
\end{enumerate}



\newpage
\section{2019-01-29-nearest-neighbors-code}
Here are $n=4$ data with $p=2$ input dimensions. Each row is a person
for which we have measured the height (first column of $X$, in
centimeters), weight (second column in pounds). The output $y$ that we
want to predict is a blood pressure measurement.

\begin{equation*}
  X = \left[\begin{array}{cc}
              170 & 140 \\
              200 & 160 \\
              180 & 200 \\
              140 & 150 
\end{array}\right],\ 
  y = \left[\begin{array}{c}
              120 \\
              115 \\
              135 \\
              140 
\end{array}\right]
\end{equation*}

1. How would you represent these data in R? (fill in the blanks)

\texttt{X <- matrix(c(%
\underline{\hspace{1cm}},\underline{\hspace{1cm}},%
\underline{\hspace{1cm}},\underline{\hspace{1cm}},%
\underline{\hspace{1cm}},\underline{\hspace{1cm}},%
\underline{\hspace{1cm}},\underline{\hspace{1cm}}),\\
  nrow=\underline{\hspace{1cm}},
  ncol=\underline{\hspace{1cm}})}

\texttt{y <- c(%
\underline{\hspace{1cm}},\underline{\hspace{1cm}},%
\underline{\hspace{1cm}},\underline{\hspace{1cm}})}

2. Using \verb|.C("myPrint_interface", as.double(X), PACKAGE="myPkg")| we can
access the inputs via a C++ function:

\begin{verbatim}
void myPrint_interface(double *X_ptr){
  ...
}
\end{verbatim}

Inside that function, what is the value of \verb|X_ptr[4]|? \underline{\hspace{1in}}

\newpage
\section{2019-01-31-coding}
Here is a block of C++ code which declares some variables that will be
used for computing the nearest neighbors predictions for a multi-class
classification problem with \verb|n_labels=10| classes. Assume there
are \verb|nrow=1000| training observations in a feature/input space of
size \verb|ncol=256|. For each line of code, indicate (1) the total
number of elements stored in the corresponding C array, (2) the
underlying C type of each of those elements, double or int, and (3)
YES if that line of code performs a dynamic memory allocation to get a
new C array, otherwise NO.
\newcommand{\oneblank}{\underline{\hspace{2in}}}
\newcommand{\blanks}{(1)\oneblank (2)\oneblank (3)\oneblank\vskip 1cm}
\begin{verbatim}
  Eigen::Map< Eigen::MatrixXd > train_inputs_mat(train_inputs_ptr, nrow, ncol);
\end{verbatim}
\blanks
\begin{verbatim}
  Eigen::Map< Eigen::VectorXd > test_input_vec(test_input_ptr, ncol);
\end{verbatim}
\blanks
\begin{verbatim}
  Eigen::VectorXd distance_vec(nrow);
\end{verbatim}
\blanks
\begin{verbatim}
  Eigen::VectorXi sorted_index_vec(nrow);
\end{verbatim}
\blanks
\begin{verbatim}
  Eigen::VectorXi label_count_vec(n_labels);
\end{verbatim}
\blanks

\newpage
\section{2019-02-05-linear-regression}
Let $w\in\mathbb R$ and $g(w) = \frac 1 2 (w-4)^2$ be a cost function
that we will minimize via gradient descent.

Derive an expression for gradient $\nabla g(w)$ in terms of $w$.

\vskip 1in 

If we start at the origin $w^{(0)}=0$, what is the starting value of the
cost function? $g(w^{(0)})=\underline{\hspace{1in}}$ 

Let's use gradient descent with step size $\alpha=0.5$ to find a lower
cost. Recall that the gradient descent updates are given by
$w^{(t)}=w^{(t-1)} - \alpha\nabla g(w^{(t-1)})$ for all
$t\in\{1, 2, \dots, \}$. What are the first two steps in gradient
descent? Fill in the blanks below. (each should be a real number)

$\nabla g(w^{(0)})$ = \underline{\hspace{1in}}

\vskip 1in

$ w^{(1)}$ = \underline{\hspace{1in}}

\vskip 1in

$\nabla g(w^{(1)})$ = \underline{\hspace{1in}}

\vskip 1in

$ w^{(2)}$ = \underline{\hspace{1in}}

\vskip 1in

What is the ending value of the cost function?
$g(w^{(2)})=\underline{\hspace{1in}}$

\newpage
\section{2019-02-07-logistic-regression}
\textbf{Poisson regression} is a  machine learning problem
where the output/label $y_i\in\{0,1, \dots \}$ is integer-valued, and
the input/features $x_i\in\mathbb R^p$ is a real vector as usual. For
example $y_i$ could be the number of pennies in your wallet, the
number of cars in your garage, or the number of books in your backpack
--- all of these are non-negative integers.

This case needs special treatment because if you use standard linear
regression, with the square loss, you end up with a prediction
function $f(x_i)\in\mathbb R$ that predicts real numbers, and it does
not make sense to predict a negative number (or a non-integer number)
of pennies/cars/books. We will derive a loss function to use in this case.

We assume $y_i\sim \text{Poisson}(\lambda_i)$ where
$\lambda_i\in\mathbb R_+$ is a non-negative real number --- it is
called the mean or rate parameter. The Poisson probability mass function is
\begin{equation*}
  \text{Pr}(y_i, \lambda_i) = \frac{\lambda_i^{y_i} e^{-\lambda_i}}{y_i!}
\end{equation*}

Derive an expression in terms of $y_i$ and $\lambda_i$ for the
log-likelihood of the mean parameter $\lambda_i$ given a single label
$y_i$: 

\vskip 1in

$\log\text{Pr}(y_i, \lambda_i)$ = \underline{\hspace{5in}}

We learn a linear function
$f(x_i)=w^T x_i = \log \lambda_i\in\mathbb R$, which means that
$\lambda_i = e^{w^T x_i}$. 

The negative log-likelihood of a particular weight vector
$w\in\mathbb R^p$ is therefore
\begin{eqnarray*}
  -\text{LogLik}(w) &=& - \sum_{i=1}^n \log\text{Pr}(y_i, e^{w^T x_i})\\
\vspace{ 1in}
  &=& \sum_{i=1}^n \underline{\hspace{5in}}
\end{eqnarray*}

In the blank above, write an expression for the negative log-likelihood in terms of $y_i,x_i,w$. There should be three terms that are
added/subtracted together. Circle the term that does NOT depend on $w$
--- the other two terms can be used as a loss function to minimize.

\newpage
\section{2019-02-12-log-reg-gradient}
\textbf{Logistic regression} is a  machine learning problem
where the output/label $\tilde y_i\in\{-1,1 \}$ is binary-valued, and
the inputs/features $x_i\in\mathbb R^p$ is a real vector as usual. 

Let $X\in \mathbb R^{n\times p}$ be the input/feature matrix, and let
$\tilde y\in\{-1,1\}^n$ be the vector of labels. Let
\begin{equation*}
  \tilde Y = \text{Diag}(\tilde y)=\left[
    \begin{array}{ccc}
      \tilde y_1& 0&0\\
      0 & \ddots & 0\\
      0 & 0 & \tilde y_n
    \end{array}
\right]\in\mathbb R^{n\times n}
\end{equation*} 
be a matrix with labels on the diagonal and zeros elsewhere.

The total logistic
loss for the linear prediction function $f(x_i)=w^T x_i$ is
\begin{equation*}
  \mathcal L(w) = \sum_{i=1}^n \log[ 1+ \exp(-\tilde y_i w^Tx_i)].
\end{equation*}
Let 
\begin{equation*}
  v = \left[
  \begin{array}{c}
    \frac{1}{1+\exp(\tilde y_1 w^T x_1)}\\
    \vdots \\ 
    \frac{1}{1+\exp(\tilde y_n w^T x_n)}
  \end{array}
\right]\in\mathbb R^n.
\end{equation*}
Derive an expression in terms of $X,\tilde Y,v$ for the gradient of the
total logistic loss and put it in the blank below.

\vskip 1in

$\nabla \mathcal L(w)= $\underline{\hspace{2in}}

\newpage
\section{2019-02-14-L2-regularization}
In the statistics literature, the ridge regression problem is
typically defined as follows. The
output/label $ y_i\in\mathbb R$ is real-valued, and the
inputs/features $x_i\in\mathbb R^p$ is a real vector as usual.
Let $X\in \mathbb R^{n\times p}$ be the input/feature matrix, and let
$ y\in\mathbb R^n$ be the vector of labels. 

The linear prediction function is
$f_{\beta,w}(x_i) = \beta + w^T x_i$, where $\beta\in\mathbb R$ is
called the ``intercept'' or ``bias'' term, and $w\in\mathbb R^p$ is
the usual vector of weights, one for each feature.

Let $1_n=\left[
  \begin{array}{ccc}
    1 &\cdots&1
  \end{array}
\right]^T\in\mathbb R^n$ be an $n$-vector of ones. The ridge
regression cost function can then be defined as
\begin{equation*}
  \mathcal C_\lambda(\beta, w) = ||1_n\beta + X w - y||_2^2 + \lambda ||w||_2^2.
\end{equation*}
Note in the definition above that L2 regularization is only used for
the weight vector $w$ (not for the bias/intercept $\beta$).

The optimal model parameters for a particular $\lambda\geq 0$ are
defined as
\begin{equation*}
  \hat \beta^\lambda, \hat w^\lambda = \argmin_{\beta\in\mathbb R, w\in\mathbb R^p}
\mathcal C_\lambda(\beta, w).
\end{equation*}
To find the optimal model parameters we must first compute the
gradients, (fill in the blanks below in terms of $X,y,w,\beta,1_n$)

\vskip 1in

$\nabla_\beta \mathcal C_\lambda(\beta, w)= $\underline{\hspace{2in}}

\vskip 1in

$\nabla_w \mathcal C_\lambda(\beta, w)= $\underline{\hspace{2in}}

\newpage
\section{2019-02-26-line-search}
Exact line search in 2 dimensions. For $w\in\mathbb R^2$, define the
cost function
$$C(w) = \frac 1 2 (w_1-1)^2 + \frac 1 2 (w_2+1)^2 = \frac 1 2 ||w +
\left[\begin{array}{c}
  -1\\
   1
\end{array}\right]
||^2_2$$

\vskip 1cm
Derive an expression for the gradient in terms of $w$, $\nabla C(w)=$\underline{\hspace{2in}}

Let $w^{(0)}=0$ be the starting point of gradient descent, at the
origin. 

\vskip 1cm
The descent direction is

$d^{(0)} = -\nabla C(w^{(0)})=$\underline{\hspace{2in}}

The cost of a step with size $\alpha>0$ in that direction is
\begin{equation*}
  \mathcal C_0(\alpha) = C(w^{(0)} + \alpha d^{(0)}).
\end{equation*}

To find the step size with the lowest cost we first need the derivative
(in terms of $\alpha$):

\vskip 1cm
\begin{equation*}
  \mathcal C'_0(\alpha) = \underline{\hspace{2in}}
\end{equation*}

\vskip 1cm 

Setting the derivative to zero, $\mathcal C'_0(\alpha)=0$,
then solving 

for $\alpha$
implies an optimal step size of
$\alpha^{(0)}=\argmin_\alpha \mathcal
C_0(\alpha)=$\underline{\hspace{2in}}

\vskip 1cm
Taking that step lands us at 

\begin{equation*}
  w^{(1)} = w^{(0)} + \alpha^{(0)} d^{(0)} = \underline{\hspace{2in}}
\end{equation*}

\vskip 1cm
which has a cost of

\begin{equation*}
  \mathcal C_0(\alpha^{(0)}) = C(w^{(1)}) = \underline{\hspace{2in}}
\end{equation*}


\newpage
\section{2019-02-28-backtracking-line-search}
\textbf{Exact line search for un-regularized least squares linear regression. }

For an input/feature matrix $X\in \mathbb R^{n\times p}$, an
output/label vector $y\in\mathbb R^n$, and a weight vector
$w\in\mathbb R^p$, define the least squares loss function
$$ L(w) = \frac 1 2|| Xw - y||^2_2$$

\vskip 1in Derive an expression for the gradient in terms of $X,y,w$.
$\nabla L(w)=$\underline{\hspace{2in}}

\vskip 1in
The descent direction at $w$ is the negative gradient,
$ d = -\nabla L(w)=$\underline{\hspace{2in}}

The cost of a step with size $\alpha>0$ in that direction is
\vskip 1in
\begin{equation*}
  \mathcal L(\alpha) = L(w + \alpha d) = \underline{\hspace{2in}}
\end{equation*}

To find the step size with the min cost we first need the derivative
(in terms of $\alpha,d,w,X,y$):

\vskip 1in
\begin{equation*}
  \mathcal L'(\alpha) = \underline{\hspace{2in}}
\end{equation*}

Setting the derivative to zero, $\mathcal L'(\alpha)=0$,  implies an optimal step size of
\vskip 1in
$\argmin_\alpha \mathcal
L'(\alpha)=$\underline{\hspace{2in}}


\end{document}
